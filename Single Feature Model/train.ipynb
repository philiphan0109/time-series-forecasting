{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "from model import Transformer # this is the transformer.py file\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>temp_anomaly_00N90N</th>\n",
       "      <th>temp_anomaly_90S90N</th>\n",
       "      <th>temp_anomaly_60S60N</th>\n",
       "      <th>temp_anomaly_20S20N</th>\n",
       "      <th>temp_anomaly_90S60S</th>\n",
       "      <th>temp_anomaly_90S20S</th>\n",
       "      <th>temp_anomaly_00N30N</th>\n",
       "      <th>temp_anomaly_20N90N</th>\n",
       "      <th>temp_anomaly_30S00N</th>\n",
       "      <th>temp_anomaly_30N60N</th>\n",
       "      <th>temp_anomaly_60S30S</th>\n",
       "      <th>temp_anomaly_90S00N</th>\n",
       "      <th>temp_anomaly_60N90N</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1880</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.428327</td>\n",
       "      <td>-0.367828</td>\n",
       "      <td>-0.385516</td>\n",
       "      <td>-0.470565</td>\n",
       "      <td>0.594499</td>\n",
       "      <td>-0.224642</td>\n",
       "      <td>-0.314540</td>\n",
       "      <td>-0.473735</td>\n",
       "      <td>-0.466897</td>\n",
       "      <td>-0.587422</td>\n",
       "      <td>-0.245249</td>\n",
       "      <td>-0.321349</td>\n",
       "      <td>-0.265796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1880</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.633566</td>\n",
       "      <td>-0.479081</td>\n",
       "      <td>-0.503620</td>\n",
       "      <td>-0.363749</td>\n",
       "      <td>1.015887</td>\n",
       "      <td>-0.279927</td>\n",
       "      <td>-0.346496</td>\n",
       "      <td>-0.859633</td>\n",
       "      <td>-0.489179</td>\n",
       "      <td>-1.127827</td>\n",
       "      <td>-0.250420</td>\n",
       "      <td>-0.342815</td>\n",
       "      <td>0.034542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1880</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.607099</td>\n",
       "      <td>-0.444028</td>\n",
       "      <td>-0.455160</td>\n",
       "      <td>-0.186829</td>\n",
       "      <td>0.409663</td>\n",
       "      <td>-0.302381</td>\n",
       "      <td>-0.090030</td>\n",
       "      <td>-0.879228</td>\n",
       "      <td>-0.410413</td>\n",
       "      <td>-1.308984</td>\n",
       "      <td>-0.228976</td>\n",
       "      <td>-0.302125</td>\n",
       "      <td>-0.378169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1880</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.382325</td>\n",
       "      <td>-0.374990</td>\n",
       "      <td>-0.385181</td>\n",
       "      <td>-0.292633</td>\n",
       "      <td>0.721319</td>\n",
       "      <td>-0.319337</td>\n",
       "      <td>0.040845</td>\n",
       "      <td>-0.506497</td>\n",
       "      <td>-0.542937</td>\n",
       "      <td>-0.842017</td>\n",
       "      <td>-0.254012</td>\n",
       "      <td>-0.368574</td>\n",
       "      <td>-0.285724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1880</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.305193</td>\n",
       "      <td>-0.415901</td>\n",
       "      <td>-0.425577</td>\n",
       "      <td>-0.477365</td>\n",
       "      <td>0.369175</td>\n",
       "      <td>-0.483292</td>\n",
       "      <td>-0.282362</td>\n",
       "      <td>-0.303430</td>\n",
       "      <td>-0.672377</td>\n",
       "      <td>-0.326057</td>\n",
       "      <td>-0.432202</td>\n",
       "      <td>-0.520792</td>\n",
       "      <td>-0.305153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1711</th>\n",
       "      <td>2022</td>\n",
       "      <td>8</td>\n",
       "      <td>0.982607</td>\n",
       "      <td>0.613061</td>\n",
       "      <td>0.569696</td>\n",
       "      <td>0.220776</td>\n",
       "      <td>0.342118</td>\n",
       "      <td>0.247630</td>\n",
       "      <td>0.476373</td>\n",
       "      <td>1.355447</td>\n",
       "      <td>0.193023</td>\n",
       "      <td>1.557818</td>\n",
       "      <td>0.221110</td>\n",
       "      <td>0.208904</td>\n",
       "      <td>1.380191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1712</th>\n",
       "      <td>2022</td>\n",
       "      <td>9</td>\n",
       "      <td>0.955873</td>\n",
       "      <td>0.594641</td>\n",
       "      <td>0.572964</td>\n",
       "      <td>0.243651</td>\n",
       "      <td>0.175700</td>\n",
       "      <td>0.213590</td>\n",
       "      <td>0.486164</td>\n",
       "      <td>1.300414</td>\n",
       "      <td>0.175039</td>\n",
       "      <td>1.572939</td>\n",
       "      <td>0.230871</td>\n",
       "      <td>0.197730</td>\n",
       "      <td>1.035945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1713</th>\n",
       "      <td>2022</td>\n",
       "      <td>10</td>\n",
       "      <td>1.003740</td>\n",
       "      <td>0.632713</td>\n",
       "      <td>0.537141</td>\n",
       "      <td>0.247111</td>\n",
       "      <td>0.527499</td>\n",
       "      <td>0.256682</td>\n",
       "      <td>0.427559</td>\n",
       "      <td>1.380687</td>\n",
       "      <td>0.212318</td>\n",
       "      <td>1.438089</td>\n",
       "      <td>0.226281</td>\n",
       "      <td>0.229366</td>\n",
       "      <td>2.217422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1714</th>\n",
       "      <td>2022</td>\n",
       "      <td>11</td>\n",
       "      <td>0.794545</td>\n",
       "      <td>0.478981</td>\n",
       "      <td>0.463640</td>\n",
       "      <td>0.206005</td>\n",
       "      <td>-0.504358</td>\n",
       "      <td>0.182402</td>\n",
       "      <td>0.530329</td>\n",
       "      <td>1.043458</td>\n",
       "      <td>0.075191</td>\n",
       "      <td>1.066647</td>\n",
       "      <td>0.299289</td>\n",
       "      <td>0.140799</td>\n",
       "      <td>1.115256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1715</th>\n",
       "      <td>2022</td>\n",
       "      <td>12</td>\n",
       "      <td>0.701498</td>\n",
       "      <td>0.466537</td>\n",
       "      <td>0.426607</td>\n",
       "      <td>0.293777</td>\n",
       "      <td>-0.541243</td>\n",
       "      <td>0.249940</td>\n",
       "      <td>0.614876</td>\n",
       "      <td>0.858875</td>\n",
       "      <td>0.197692</td>\n",
       "      <td>0.550641</td>\n",
       "      <td>0.358549</td>\n",
       "      <td>0.220451</td>\n",
       "      <td>1.723837</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1716 rows Ã— 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      year  month  temp_anomaly_00N90N  temp_anomaly_90S90N  \\\n",
       "0     1880      1            -0.428327            -0.367828   \n",
       "1     1880      2            -0.633566            -0.479081   \n",
       "2     1880      3            -0.607099            -0.444028   \n",
       "3     1880      4            -0.382325            -0.374990   \n",
       "4     1880      5            -0.305193            -0.415901   \n",
       "...    ...    ...                  ...                  ...   \n",
       "1711  2022      8             0.982607             0.613061   \n",
       "1712  2022      9             0.955873             0.594641   \n",
       "1713  2022     10             1.003740             0.632713   \n",
       "1714  2022     11             0.794545             0.478981   \n",
       "1715  2022     12             0.701498             0.466537   \n",
       "\n",
       "      temp_anomaly_60S60N  temp_anomaly_20S20N  temp_anomaly_90S60S  \\\n",
       "0               -0.385516            -0.470565             0.594499   \n",
       "1               -0.503620            -0.363749             1.015887   \n",
       "2               -0.455160            -0.186829             0.409663   \n",
       "3               -0.385181            -0.292633             0.721319   \n",
       "4               -0.425577            -0.477365             0.369175   \n",
       "...                   ...                  ...                  ...   \n",
       "1711             0.569696             0.220776             0.342118   \n",
       "1712             0.572964             0.243651             0.175700   \n",
       "1713             0.537141             0.247111             0.527499   \n",
       "1714             0.463640             0.206005            -0.504358   \n",
       "1715             0.426607             0.293777            -0.541243   \n",
       "\n",
       "      temp_anomaly_90S20S  temp_anomaly_00N30N  temp_anomaly_20N90N  \\\n",
       "0               -0.224642            -0.314540            -0.473735   \n",
       "1               -0.279927            -0.346496            -0.859633   \n",
       "2               -0.302381            -0.090030            -0.879228   \n",
       "3               -0.319337             0.040845            -0.506497   \n",
       "4               -0.483292            -0.282362            -0.303430   \n",
       "...                   ...                  ...                  ...   \n",
       "1711             0.247630             0.476373             1.355447   \n",
       "1712             0.213590             0.486164             1.300414   \n",
       "1713             0.256682             0.427559             1.380687   \n",
       "1714             0.182402             0.530329             1.043458   \n",
       "1715             0.249940             0.614876             0.858875   \n",
       "\n",
       "      temp_anomaly_30S00N  temp_anomaly_30N60N  temp_anomaly_60S30S  \\\n",
       "0               -0.466897            -0.587422            -0.245249   \n",
       "1               -0.489179            -1.127827            -0.250420   \n",
       "2               -0.410413            -1.308984            -0.228976   \n",
       "3               -0.542937            -0.842017            -0.254012   \n",
       "4               -0.672377            -0.326057            -0.432202   \n",
       "...                   ...                  ...                  ...   \n",
       "1711             0.193023             1.557818             0.221110   \n",
       "1712             0.175039             1.572939             0.230871   \n",
       "1713             0.212318             1.438089             0.226281   \n",
       "1714             0.075191             1.066647             0.299289   \n",
       "1715             0.197692             0.550641             0.358549   \n",
       "\n",
       "      temp_anomaly_90S00N  temp_anomaly_60N90N  \n",
       "0               -0.321349            -0.265796  \n",
       "1               -0.342815             0.034542  \n",
       "2               -0.302125            -0.378169  \n",
       "3               -0.368574            -0.285724  \n",
       "4               -0.520792            -0.305153  \n",
       "...                   ...                  ...  \n",
       "1711             0.208904             1.380191  \n",
       "1712             0.197730             1.035945  \n",
       "1713             0.229366             2.217422  \n",
       "1714             0.140799             1.115256  \n",
       "1715             0.220451             1.723837  \n",
       "\n",
       "[1716 rows x 15 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data preprocessing\n",
    "\n",
    "from data_processing import process_file\n",
    "\n",
    "data_path = \"data/\"\n",
    "\n",
    "all_data = pd.DataFrame()\n",
    "\n",
    "for file in os.listdir(data_path):\n",
    "    if file.endswith(\".txt\"):\n",
    "        file_path = os.path.join(data_path, file)\n",
    "        loc_df = process_file(file_path)\n",
    "\n",
    "        # print(file, len(loc_df))\n",
    "        if len(all_data) == 0:\n",
    "            all_data = loc_df\n",
    "        else:\n",
    "            all_data = pd.merge(all_data, loc_df, on = [\"year\", \"month\"])\n",
    "\n",
    "all_data.to_csv(\"data/processed_data.csv\")\n",
    "all_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of segments: 132\n",
      "Length of each input vector (months): 120\n",
      "Length of each target vector a(months): 12\n",
      "\n",
      "Tensor Lengths:\n",
      "There are 132 input tensors and 132 target tensors\n",
      "Input tensors length: 120\n",
      "Target tensors length: 12\n"
     ]
    }
   ],
   "source": [
    "# Segmentation\n",
    "\n",
    "from data_processing import create_segments\n",
    "\n",
    "input_years = 10  # Length of input period in years\n",
    "target_years = 1   # Length of target period in years\n",
    "\n",
    "overlapping = True\n",
    "input_segments, target_segments = create_segments(all_data, input_years, target_years, overlapping=overlapping)\n",
    "\n",
    "print(\"# of segments:\", len(input_segments))\n",
    "print(\"Length of each input vector (months):\", len(input_segments[0]))\n",
    "print(\"Length of each target vector a(months):\", len(target_segments[0]))\n",
    "\n",
    "# Convert to tensors\n",
    "feature_columns = all_data.columns.difference(['year', 'month'], sort=False)\n",
    "input_tensors = torch.tensor([df[feature_columns].values for df in input_segments], dtype=torch.float32)\n",
    "target_tensors = torch.tensor([df[feature_columns].values for df in target_segments], dtype=torch.float32)\n",
    "\n",
    "# Print tensor lengths\n",
    "print(\"\\nTensor Lengths:\")\n",
    "print(f\"There are {len(input_tensors)} input tensors and {len(target_tensors)} target tensors\")\n",
    "print(f\"Input tensors length: {len(input_tensors[0])}\")\n",
    "print(f\"Target tensors length: {len(target_tensors[0])}\")\n",
    "\n",
    "# Prepare the data to be saved\n",
    "data_to_save = {\n",
    "    'input_tensors': input_tensors,\n",
    "    'target_tensors': target_tensors\n",
    "\n",
    "}\n",
    "# Dumping input and target arrays into a pickle file\n",
    "with open('data/tensor_data.pkl', 'wb') as f:\n",
    "    pickle.dump(data_to_save, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loader Length: 79\n",
      "Validation Loader Length: 26\n",
      "Test Loader Length: 27\n",
      "Input batch shape: torch.Size([1, 120, 13])\n",
      "Target batch shape: torch.Size([1, 12, 13])\n"
     ]
    }
   ],
   "source": [
    "from dataset import TimeSeriesDataset\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "batch_size = 1\n",
    "\n",
    "ts_dataset = TimeSeriesDataset(input_tensors, target_tensors)\n",
    "train_size = int(0.6 * len(ts_dataset))  # e.g., 70% of data for training\n",
    "val_size = int(0.2 * len(ts_dataset))  # 20% of data for validation \n",
    "test_size = len(ts_dataset) - train_size - val_size  # Remaining for testing\n",
    "\n",
    "# Randomly split the dataset into training and validation datasets\n",
    "train_dataset, val_dataset, test_dataset = random_split(ts_dataset, [train_size, val_size, test_size])\n",
    "\n",
    "# Create DataLoaders for both training and validation sets\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)  # Usually, no need to shuffle the validation set\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"Train Loader Length: {len(train_loader)}\")\n",
    "print(f\"Validation Loader Length: {len(val_loader)}\")\n",
    "print(f\"Test Loader Length: {len(test_loader)}\")\n",
    "\n",
    "input_batch, target_batch = next(iter(train_loader))\n",
    "\n",
    "# Print the shapes\n",
    "print(f'Input batch shape: {input_batch.shape}')  # e.g., torch.Size([8, 120, 15])\n",
    "print(f'Target batch shape: {target_batch.shape}')  # e.g., torch.Size([8, 12, 15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "def testing_metrics(logits_batch, target_batch):\n",
    "    mse = torch.mean((logits_batch - target_batch) ** 2)\n",
    "    rmse = torch.sqrt(mse)\n",
    "\n",
    "    mae = torch.mean(torch.abs(logits_batch - target_batch))\n",
    "\n",
    "    r_squared_values = []\n",
    "    for t in range(logits_batch.shape[1]):  # Iterate over time steps\n",
    "        logits_t = logits_batch[:, t, :].view(-1).detach().numpy()\n",
    "        target_t = target_batch[:, t, :].view(-1).detach().numpy()\n",
    "        r_squared_t = r2_score(target_t, logits_t)\n",
    "        r_squared_values.append(r_squared_t)\n",
    "\n",
    "    r_squared_avg = sum(r_squared_values) / len(r_squared_values)\n",
    "\n",
    "    return rmse.item(), mae.item(), r_squared_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "\n",
    "d_data = 13\n",
    "d_model = 15\n",
    "ffn_hidden = 2048\n",
    "num_heads = 5\n",
    "drop_prob = 0.1\n",
    "num_layers = 1\n",
    "\n",
    "transformer = Transformer(d_model, d_data, ffn_hidden, num_heads, drop_prob, num_layers)\n",
    "ar_transformer = Transformer(d_model, d_data, ffn_hidden, num_heads, drop_prob, num_layers)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.AdamW(params=transformer.parameters(), lr = 1e-3)\n",
    "ar_optimizer = torch.optim.AdamW(params = ar_transformer.parameters(), lr = 1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multistep Forward Pass\n",
    "\n",
    "def MultiStepForwardPass(model, input_batch, target_batch):\n",
    "    predictions = None  # Initialize predictions\n",
    "\n",
    "    for i in range(len(target_batch[0])):\n",
    "        if i == 0:\n",
    "            single_step_input = input_batch\n",
    "        else:\n",
    "            single_step_input = torch.cat((input_batch[:, i:, :], predictions), dim=1)\n",
    "\n",
    "        # Extract the target for the current step\n",
    "        single_step_target = target_batch[:, i:i+1, :]\n",
    "\n",
    "        # Forward pass\n",
    "        single_step_logits = model(single_step_input, single_step_target)\n",
    "\n",
    "        # Update predictions\n",
    "        if i == 0:\n",
    "            predictions = single_step_logits\n",
    "        else:\n",
    "            predictions = torch.cat((predictions, single_step_logits), dim=1)\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Training Loop\n",
    "\n",
    "def train_model(model, epochs, optimizer, criterion, save_freq = 20, auto_reg = False, print_results = True):\n",
    "    print(\"Autoregressive Model Training\") if auto_reg else print(\"Single-Shot Model Training\")\n",
    "    for epoch in range(1, epochs+1):\n",
    "        \n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_rmse = 0\n",
    "        train_mae = 0\n",
    "        train_r_squared = 0\n",
    "\n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            if auto_reg:\n",
    "                logits_batch = MultiStepForwardPass(model, input_batch, target_batch)\n",
    "            else:\n",
    "                logits_batch = model(input_batch, target_batch)\n",
    "            \n",
    "            loss = criterion(logits_batch, target_batch)\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            rmse, mae, r_squared = testing_metrics(logits_batch, target_batch)\n",
    "            train_rmse += rmse\n",
    "            train_mae += mae\n",
    "            train_r_squared += r_squared\n",
    "\n",
    "        # Calculate average loss and mean average percent error for the epoch\n",
    "        train_loss /= len(train_loader)\n",
    "        train_rmse /= len(train_loader)\n",
    "        train_mae /= len(train_loader)\n",
    "        train_r_squared /= len(train_loader)\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        val_loss = 0\n",
    "        val_rmse = 0\n",
    "        val_mae = 0\n",
    "        val_r_squared = 0\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            for input_batch, target_batch in val_loader:\n",
    "\n",
    "                # Forward pass\n",
    "                if auto_reg:\n",
    "                    logits_batch = MultiStepForwardPass(model, input_batch, target_batch)\n",
    "                else:\n",
    "                    logits_batch = model(input_batch, target_batch)\n",
    "\n",
    "                # Calculate loss\n",
    "                loss = criterion(logits_batch, target_batch)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                rmse, mae, r_squared = testing_metrics(logits_batch, target_batch)\n",
    "                val_rmse += rmse\n",
    "                val_mae += mae\n",
    "                val_r_squared += r_squared\n",
    "            \n",
    "        val_loss /= len(val_loader)\n",
    "        val_rmse /= len(val_loader)\n",
    "        val_mae /= len(val_loader)\n",
    "        val_r_squared /= len(val_loader)\n",
    "        \n",
    "        # Print epoch stats\n",
    "        if(epoch % save_freq == 0 and print_results):\n",
    "            print(f'Epoch {epoch}/{epochs} | Train Loss: {train_loss:.4f} | Train RMSE: {train_rmse:.4f} | Train MAE: {train_mae:.4f} | Train R^2: {train_r_squared:.4f}')\n",
    "            print(f'Validation Loss: {val_loss:.4f} | Validation RMSE: {val_rmse:.4f} | Validation MAE: {val_mae:.4f} | Validation R^2: {val_r_squared:.4f}')\n",
    "            print(\"\")\n",
    "    \n",
    "    # if auto_reg:\n",
    "    #     torch.save(model.state_dict(), f'saved_models/ar_transformer.pt')\n",
    "    # else:\n",
    "    #     torch.save(model.state_dict(), f'saved_models/transformer.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Testing Loop\n",
    "\n",
    "def test_model(model, test_loader, criterion, auto_reg = False):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    test_rmse = 0\n",
    "    test_mae = 0\n",
    "    test_r_squared = 0\n",
    "    all_inputs = []\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for input_batch, target_batch in test_loader:\n",
    "\n",
    "            if auto_reg:\n",
    "                logits_batch = MultiStepForwardPass(model, input_batch, target_batch)\n",
    "            else:\n",
    "                logits_batch = model(input_batch, target_batch)\n",
    "            loss = criterion(logits_batch, target_batch)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            rmse, mae, r_squared = testing_metrics(logits_batch, target_batch)\n",
    "            test_rmse += rmse\n",
    "            test_mae += mae\n",
    "            test_r_squared += r_squared\n",
    "\n",
    "            all_inputs.extend(input_batch.cpu().numpy())\n",
    "            all_predictions.extend(logits_batch.cpu().numpy())\n",
    "            all_targets.extend(target_batch.cpu().numpy())\n",
    "\n",
    "    test_loss /= len(test_loader)\n",
    "    test_rmse /= len(test_loader)\n",
    "    test_mae /= len(test_loader)\n",
    "    test_r_squared /= len(test_loader)\n",
    "\n",
    "    return test_loss, test_rmse, test_mae, test_r_squared, all_inputs, all_predictions, all_targets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training & Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single-Shot Model Training\n",
      "Epoch 20/100 | Train Loss: 0.0013 | Train RMSE: 0.0333 | Train MAE: 0.0238 | Train R^2: 0.9824\n",
      "Validation Loss: 0.0010 | Validation RMSE: 0.0304 | Validation MAE: 0.0226 | Validation R^2: 0.9848\n",
      "\n",
      "Epoch 40/100 | Train Loss: 0.0016 | Train RMSE: 0.0364 | Train MAE: 0.0269 | Train R^2: 0.9748\n",
      "Validation Loss: 0.0010 | Validation RMSE: 0.0303 | Validation MAE: 0.0239 | Validation R^2: 0.9831\n",
      "\n",
      "Epoch 60/100 | Train Loss: 0.0006 | Train RMSE: 0.0222 | Train MAE: 0.0157 | Train R^2: 0.9922\n",
      "Validation Loss: 0.0005 | Validation RMSE: 0.0203 | Validation MAE: 0.0153 | Validation R^2: 0.9932\n",
      "\n",
      "Epoch 80/100 | Train Loss: 0.0006 | Train RMSE: 0.0230 | Train MAE: 0.0168 | Train R^2: 0.9894\n",
      "Validation Loss: 0.0006 | Validation RMSE: 0.0221 | Validation MAE: 0.0162 | Validation R^2: 0.9934\n",
      "\n",
      "Epoch 100/100 | Train Loss: 0.0005 | Train RMSE: 0.0208 | Train MAE: 0.0155 | Train R^2: 0.9931\n",
      "Validation Loss: 0.0003 | Validation RMSE: 0.0174 | Validation MAE: 0.0132 | Validation R^2: 0.9953\n",
      "\n",
      "Autoregressive Model Training\n",
      "Epoch 20/100 | Train Loss: 0.0014 | Train RMSE: 0.0347 | Train MAE: 0.0251 | Train R^2: 0.9805\n",
      "Validation Loss: 0.0011 | Validation RMSE: 0.0317 | Validation MAE: 0.0227 | Validation R^2: 0.9848\n",
      "\n",
      "Epoch 40/100 | Train Loss: 0.0011 | Train RMSE: 0.0318 | Train MAE: 0.0237 | Train R^2: 0.9822\n",
      "Validation Loss: 0.0021 | Validation RMSE: 0.0423 | Validation MAE: 0.0314 | Validation R^2: 0.9746\n",
      "\n",
      "Epoch 60/100 | Train Loss: 0.0009 | Train RMSE: 0.0278 | Train MAE: 0.0201 | Train R^2: 0.9866\n",
      "Validation Loss: 0.0009 | Validation RMSE: 0.0291 | Validation MAE: 0.0225 | Validation R^2: 0.9827\n",
      "\n",
      "Epoch 80/100 | Train Loss: 0.0004 | Train RMSE: 0.0183 | Train MAE: 0.0131 | Train R^2: 0.9945\n",
      "Validation Loss: 0.0007 | Validation RMSE: 0.0252 | Validation MAE: 0.0185 | Validation R^2: 0.9877\n",
      "\n",
      "Epoch 100/100 | Train Loss: 0.0007 | Train RMSE: 0.0240 | Train MAE: 0.0171 | Train R^2: 0.9884\n",
      "Validation Loss: 0.0007 | Validation RMSE: 0.0240 | Validation MAE: 0.0167 | Validation R^2: 0.9902\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Training the model\n",
    "\n",
    "train_model(transformer, 100, optimizer, criterion, 20, auto_reg=False, print_results=True)\n",
    "train_model(ar_transformer, 100, ar_optimizer, criterion, 20, auto_reg=True, print_results=True)\n",
    "\n",
    "single_shot_results = test_model(transformer, test_loader, criterion)\n",
    "ar_results = test_model(ar_transformer, test_loader, criterion, auto_reg=True)\n",
    "\n",
    "if overlapping:\n",
    "    torch.save(transformer.state_dict(), f'saved_models/transformer.pt')\n",
    "    torch.save(ar_transformer.state_dict(), f'saved_models/ar_transformer.pt')\n",
    "else:\n",
    "    torch.save(transformer.state_dict(), f'saved_models/transformer_no_overlap.pt')\n",
    "    torch.save(ar_transformer.state_dict(), f'saved_models/ar_transformer_no_overlap.pt')\n",
    "    \n",
    "\n",
    "results = {\n",
    "    'single_shot': single_shot_results,\n",
    "    'auto_regressive': ar_results\n",
    "}\n",
    "\n",
    "if not overlapping:\n",
    "    with open('results/no_overlap.pkl', 'wb') as f:\n",
    "        pickle.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Transfer\n",
    "\n",
    "# Function to test the model and get metrics\n",
    "def test_and_get_metrics(model, loader, criterion, auto_reg=False):\n",
    "    test_loss, test_rmse, test_mae, test_r_squared, inputs, predictions, targets = test_model(model, loader, criterion, auto_reg)\n",
    "    test_results = {\n",
    "        'loss': test_loss,\n",
    "        'rmse': test_rmse,\n",
    "        'mae': test_mae,\n",
    "        'r_squared': test_r_squared,\n",
    "        'inputs': inputs,\n",
    "        'predictions': predictions,\n",
    "        'targets': targets\n",
    "    }\n",
    "    return test_results\n",
    "\n",
    "# Testing single-shot model\n",
    "single_shot_test_results = test_and_get_metrics(transformer, test_loader, criterion)\n",
    "with open('results/single_shot_test_results.pkl', 'wb') as f:\n",
    "    pickle.dump(single_shot_test_results, f)\n",
    "\n",
    "# Testing autoregressive model\n",
    "ar_test_results = test_and_get_metrics(ar_transformer, test_loader, criterion, auto_reg=True)\n",
    "with open('results/auto_reg_test_results.pkl', 'wb') as f:\n",
    "    pickle.dump(ar_test_results, f)\n",
    "\n",
    "# Testing autoregressive with single-shot model\n",
    "auto_reg_single_shot_test_results = test_and_get_metrics(transformer, test_loader, criterion, auto_reg=True)\n",
    "with open('results/auto_reg_single_shot_test_results.pkl', 'wb') as f:\n",
    "    pickle.dump(auto_reg_single_shot_test_results, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding Optimal Model Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize Number of Layers\n",
    "\n",
    "def optimize_model(num_layers_range, epochs, train_loader, val_loader, test_loader, criterion, save_freq=20):\n",
    "    return\n",
    "    results = []\n",
    "\n",
    "    for i in range(1,11):\n",
    "        print(f\"Testing with {i} layers...\")\n",
    "\n",
    "        transformer = Transformer(d_model, ffn_hidden, num_heads, drop_prob, i)\n",
    "        ar_transformer = Transformer(d_model, ffn_hidden, num_heads, drop_prob, i)\n",
    "\n",
    "        optimizer = torch.optim.AdamW(params=transformer.parameters(), lr=1e-3)\n",
    "        ar_optimizer = torch.optim.AdamW(params=ar_transformer.parameters(), lr=1e-3)\n",
    "\n",
    "        train_model(transformer, epochs, optimizer, criterion, save_freq, auto_reg=False, print_results=False)\n",
    "        train_model(ar_transformer, epochs, ar_optimizer, criterion, save_freq, auto_reg=True, print_results=False)\n",
    "\n",
    "        single_shot_results = test_model(transformer, test_loader, criterion)\n",
    "        ar_results = test_model(ar_transformer, test_loader, criterion, auto_reg=True)\n",
    "\n",
    "        result = {\n",
    "            'num_layers': i,\n",
    "            'single_shot': single_shot_results,\n",
    "            'auto_regressive': ar_results\n",
    "        }\n",
    "        results.append(result)\n",
    "\n",
    "        torch.save(transformer.state_dict(), f'saved_models/transformer_{i}_layers.pt')\n",
    "        torch.save(ar_transformer.state_dict(), f'saved_models/ar_transformer_{i}_layers.pt')\n",
    "    \n",
    "    with open('optimization_results.pkl', 'wb') as f:\n",
    "        pickle.dump(results, f)\n",
    "\n",
    "    return results\n",
    "\n",
    "num_layers_range = range(1, 11)\n",
    "epochs = 100  # or whatever number of epochs you deem appropriate\n",
    "optimization_results = optimize_model(num_layers_range, epochs, train_loader, val_loader, test_loader, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
