{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "from model import Transformer # this is the transformer.py file\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padded Greenhouse Gas Data:  (142, 14)\n",
      "Average Annual Temperature Data:  (142, 14)\n"
     ]
    }
   ],
   "source": [
    "from data_processing import create_segments\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "all_ghg_data = pd.read_csv(\"data/total-ghg-emissions.csv\")\n",
    "all_ghg_data.rename(columns={'Year': 'year'}, inplace=True)\n",
    "scaler = StandardScaler()\n",
    "all_ghg_data.iloc[:, -1] = scaler.fit_transform(all_ghg_data.iloc[:, -1].values.reshape(-1, 1)).flatten()\n",
    "all_temp_data = pd.read_csv(\"data/processed_data.csv\")\n",
    "all_temp_data = all_temp_data[all_temp_data['year'] != all_temp_data['year'].max()]\n",
    "\n",
    "annual_avg_temp = all_temp_data.groupby('year').mean().reset_index()\n",
    "annual_avg_temp.drop(columns=['Unnamed: 0', 'month'], inplace=True)\n",
    "\n",
    "\n",
    "def pad_features(df, max_features):\n",
    "    additional_cols = max_features - df.shape[1]\n",
    "    for i in range(additional_cols):\n",
    "        df[f'pad_feature_{i}'] = 0\n",
    "    return df\n",
    "\n",
    "max_features = max(all_ghg_data.shape[1], annual_avg_temp.shape[1])\n",
    "all_ghg_data.drop(columns=['Entity', 'Code'], inplace=True)\n",
    "all_ghg_data = pad_features(all_ghg_data, max_features)\n",
    "\n",
    "print(\"Padded Greenhouse Gas Data: \", all_ghg_data.shape)\n",
    "print(\"Average Annual Temperature Data: \", annual_avg_temp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GHG Input Segment Shape: (10, 14)\n",
      "Temperature Input Segment Shape: (10, 14)\n",
      "GHG Target Segment Shape: (5, 14)\n",
      "Temperature Target Segment Shape: (5, 14)\n",
      "\n",
      "Combined Input Tensor Shape: torch.Size([128, 2, 10, 13])\n",
      "Combined Target Tensor Shape: torch.Size([128, 2, 5, 13])\n"
     ]
    }
   ],
   "source": [
    "ghg_data_padded, temp_data_padded = all_ghg_data, annual_avg_temp\n",
    "\n",
    "input_years = 10\n",
    "target_years = 5\n",
    "overlapping = True\n",
    "\n",
    "# Segment the data\n",
    "ghg_input_segments, ghg_target_segments = create_segments(ghg_data_padded, input_years, target_years, overlapping)\n",
    "temp_input_segments, temp_target_segments = create_segments(temp_data_padded, input_years, target_years, overlapping)\n",
    "\n",
    "print(\"GHG Input Segment Shape:\", ghg_input_segments[0].shape)\n",
    "print(\"Temperature Input Segment Shape:\", temp_input_segments[0].shape)\n",
    "print(\"GHG Target Segment Shape:\", ghg_target_segments[0].shape)\n",
    "print(\"Temperature Target Segment Shape:\", temp_target_segments[0].shape)\n",
    "\n",
    "# Iterate over the segments, drop 'year', combine and convert to tensors\n",
    "# Temperature data is now at index 0 and GHG data at index 1\n",
    "input_segments_tensors = []\n",
    "target_segments_tensors = []\n",
    "\n",
    "for temp_segment, ghg_segment in zip(temp_input_segments, ghg_input_segments):\n",
    "    temp_tensor = torch.tensor(temp_segment.drop(columns=['year']).values, dtype=torch.float32)\n",
    "    ghg_tensor = torch.tensor(ghg_segment.drop(columns=['year']).values, dtype=torch.float32)\n",
    "    combined_input_tensor = torch.stack((temp_tensor, ghg_tensor), dim=0)\n",
    "    input_segments_tensors.append(combined_input_tensor)\n",
    "\n",
    "for temp_segment, ghg_segment in zip(temp_target_segments, ghg_target_segments):\n",
    "    temp_tensor = torch.tensor(temp_segment.drop(columns=['year']).values, dtype=torch.float32)\n",
    "    ghg_tensor = torch.tensor(ghg_segment.drop(columns=['year']).values, dtype=torch.float32)\n",
    "    combined_target_tensor = torch.stack((temp_tensor, ghg_tensor), dim=0)\n",
    "    target_segments_tensors.append(combined_target_tensor)\n",
    "\n",
    "# Convert lists of combined segments into tensors\n",
    "input_tensors = torch.stack(input_segments_tensors, dim=0)\n",
    "target_tensors = torch.stack(target_segments_tensors, dim=0)\n",
    "\n",
    "print(\"\\nCombined Input Tensor Shape:\", input_tensors.shape)\n",
    "print(\"Combined Target Tensor Shape:\", target_tensors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loader Length: 19\n",
      "Validation Loader Length: 6\n",
      "Test Loader Length: 6\n",
      "Input batch shape: torch.Size([4, 2, 10, 13])\n",
      "Target batch shape: torch.Size([4, 2, 5, 13])\n"
     ]
    }
   ],
   "source": [
    "from dataset import TimeSeriesDataset\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "batch_size = 4\n",
    "\n",
    "ts_dataset = TimeSeriesDataset(input_tensors, target_tensors)\n",
    "train_size = int(0.6 * len(ts_dataset))  # e.g., 70% of data for training\n",
    "val_size = int(0.2 * len(ts_dataset))  # 20% of data for validation \n",
    "test_size = len(ts_dataset) - train_size - val_size  # Remaining for testing\n",
    "\n",
    "# Randomly split the dataset into training and validation datasets\n",
    "train_dataset, val_dataset, test_dataset = random_split(ts_dataset, [train_size, val_size, test_size])\n",
    "\n",
    "# Create DataLoaders for both training and validation sets\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, drop_last=True)  # Usually, no need to shuffle the validation set\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "print(f\"Train Loader Length: {len(train_loader)}\")\n",
    "print(f\"Validation Loader Length: {len(val_loader)}\")\n",
    "print(f\"Test Loader Length: {len(test_loader)}\")\n",
    "\n",
    "input_batch, target_batch = next(iter(train_loader))\n",
    "\n",
    "# Print the shapes\n",
    "print(f'Input batch shape: {input_batch.shape}')  # e.g., torch.Size([8, 120, 15])\n",
    "print(f'Target batch shape: {target_batch.shape}')  # e.g., torch.Size([8, 12, 15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "def testing_metrics(logits_batch, target_batch):\n",
    "    mse = torch.mean((logits_batch - target_batch) ** 2)\n",
    "    rmse = torch.sqrt(mse)\n",
    "    mae = torch.mean(torch.abs(logits_batch - target_batch))\n",
    "\n",
    "    r_squared_values = []\n",
    "    for t in range(logits_batch.shape[1]):  # Iterate over time steps\n",
    "        logits_t = logits_batch[:, t, :].reshape(-1).detach().numpy()\n",
    "        target_t = target_batch[:, t, :].reshape(-1).detach().numpy()\n",
    "        r_squared_t = r2_score(target_t, logits_t)\n",
    "        r_squared_values.append(r_squared_t)\n",
    "\n",
    "    r_squared_avg = sum(r_squared_values) / len(r_squared_values)\n",
    "    return rmse.item(), mae.item(), r_squared_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "\n",
    "d_temp = 13\n",
    "d_ghg = 13\n",
    "d_enc = 15\n",
    "d_dec = 30\n",
    "d_data = 26\n",
    "ffn_hidden = 2048\n",
    "num_heads = 5\n",
    "drop_prob = 0.1\n",
    "num_layers = 1\n",
    "\n",
    "transformer = Transformer(d_enc, d_dec, d_data, d_temp, d_ghg, ffn_hidden, num_heads, drop_prob, num_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Training Loop\n",
    "\n",
    "def train_model(model, epochs, optimizer, criterion, save_freq = 20, print_results = True):\n",
    "    print(\"Single-Shot Model Training\")\n",
    "    for epoch in range(1, epochs+1):\n",
    "        \n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_rmse = 0\n",
    "        train_mae = 0\n",
    "        train_r_squared = 0\n",
    "\n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            logits_batch = model(input_batch, target_batch)\n",
    "\n",
    "            target_batch = target_batch.permute(0, 2, 1, 3)\n",
    "            target_batch = target_batch.reshape(batch_size, -1, 26)\n",
    "            \n",
    "            loss = criterion(logits_batch, target_batch)\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            rmse, mae, r_squared = testing_metrics(logits_batch, target_batch)\n",
    "            train_rmse += rmse\n",
    "            train_mae += mae\n",
    "            train_r_squared += r_squared\n",
    "\n",
    "        # Calculate average loss and mean average percent error for the epoch\n",
    "        train_loss /= len(train_loader)\n",
    "        train_rmse /= len(train_loader)\n",
    "        train_mae /= len(train_loader)\n",
    "        train_r_squared /= len(train_loader)\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        val_loss = 0\n",
    "        val_rmse = 0\n",
    "        val_mae = 0\n",
    "        val_r_squared = 0\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            for input_batch, target_batch in val_loader:\n",
    "\n",
    "                target_batch = target_batch.permute(0, 2, 1, 3)\n",
    "                target_batch = target_batch.reshape(batch_size, -1, 26)\n",
    "\n",
    "                # Calculate loss\n",
    "                loss = criterion(logits_batch, target_batch)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                rmse, mae, r_squared = testing_metrics(logits_batch, target_batch)\n",
    "                val_rmse += rmse\n",
    "                val_mae += mae\n",
    "                val_r_squared += r_squared\n",
    "            \n",
    "        val_loss /= len(val_loader)\n",
    "        val_rmse /= len(val_loader)\n",
    "        val_mae /= len(val_loader)\n",
    "        val_r_squared /= len(val_loader)\n",
    "        \n",
    "        # Print epoch stats\n",
    "        if(epoch % save_freq == 0 and print_results):\n",
    "            print(f'Epoch {epoch}/{epochs} | Train Loss: {train_loss:.4f} | Train RMSE: {train_rmse:.4f} | Train MAE: {train_mae:.4f} | Train R^2: {train_r_squared:.4f}')\n",
    "            print(f'Validation Loss: {val_loss:.4f} | Validation RMSE: {val_rmse:.4f} | Validation MAE: {val_mae:.4f} | Validation R^2: {val_r_squared:.4f}')\n",
    "            print(\"\")\n",
    "    \n",
    "        torch.save(model.state_dict(), f'saved_models/transformer.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single-Shot Model Training\n",
      "Epoch 20/1000 | Train Loss: 0.0355 | Train RMSE: 0.1880 | Train MAE: 0.1432 | Train R^2: 0.6917\n",
      "Validation Loss: 0.2784 | Validation RMSE: 0.5023 | Validation MAE: 0.3375 | Validation R^2: -1.8826\n",
      "\n",
      "Epoch 40/1000 | Train Loss: 0.0251 | Train RMSE: 0.1575 | Train MAE: 0.1191 | Train R^2: 0.7980\n",
      "Validation Loss: 0.2329 | Validation RMSE: 0.4611 | Validation MAE: 0.3087 | Validation R^2: -1.2512\n",
      "\n",
      "Epoch 60/1000 | Train Loss: 0.0204 | Train RMSE: 0.1426 | Train MAE: 0.1081 | Train R^2: 0.8186\n",
      "Validation Loss: 0.1650 | Validation RMSE: 0.4011 | Validation MAE: 0.2634 | Validation R^2: -0.6079\n",
      "\n",
      "Epoch 80/1000 | Train Loss: 0.0169 | Train RMSE: 0.1296 | Train MAE: 0.0984 | Train R^2: 0.8515\n",
      "Validation Loss: 0.2372 | Validation RMSE: 0.4768 | Validation MAE: 0.2888 | Validation R^2: -1.4209\n",
      "\n",
      "Epoch 100/1000 | Train Loss: 0.0145 | Train RMSE: 0.1203 | Train MAE: 0.0924 | Train R^2: 0.8749\n",
      "Validation Loss: 0.2742 | Validation RMSE: 0.4973 | Validation MAE: 0.3230 | Validation R^2: -1.8364\n",
      "\n",
      "Epoch 120/1000 | Train Loss: 0.0126 | Train RMSE: 0.1118 | Train MAE: 0.0861 | Train R^2: 0.8960\n",
      "Validation Loss: 0.2371 | Validation RMSE: 0.4627 | Validation MAE: 0.2986 | Validation R^2: -1.4901\n",
      "\n",
      "Epoch 140/1000 | Train Loss: 0.0112 | Train RMSE: 0.1056 | Train MAE: 0.0812 | Train R^2: 0.9054\n",
      "Validation Loss: 0.3251 | Validation RMSE: 0.5591 | Validation MAE: 0.3473 | Validation R^2: -2.1749\n",
      "\n",
      "Epoch 160/1000 | Train Loss: 0.0096 | Train RMSE: 0.0979 | Train MAE: 0.0753 | Train R^2: 0.9140\n",
      "Validation Loss: 0.1771 | Validation RMSE: 0.4122 | Validation MAE: 0.2500 | Validation R^2: -0.7008\n",
      "\n",
      "Epoch 180/1000 | Train Loss: 0.0091 | Train RMSE: 0.0953 | Train MAE: 0.0739 | Train R^2: 0.9151\n",
      "Validation Loss: 0.2520 | Validation RMSE: 0.4851 | Validation MAE: 0.2827 | Validation R^2: -1.3666\n",
      "\n",
      "Epoch 200/1000 | Train Loss: 0.0080 | Train RMSE: 0.0894 | Train MAE: 0.0694 | Train R^2: 0.9303\n",
      "Validation Loss: 0.3033 | Validation RMSE: 0.5334 | Validation MAE: 0.3388 | Validation R^2: -2.1966\n",
      "\n",
      "Epoch 220/1000 | Train Loss: 0.0080 | Train RMSE: 0.0890 | Train MAE: 0.0691 | Train R^2: 0.9240\n",
      "Validation Loss: 0.1318 | Validation RMSE: 0.3496 | Validation MAE: 0.2224 | Validation R^2: -0.1746\n",
      "\n",
      "Epoch 240/1000 | Train Loss: 0.0070 | Train RMSE: 0.0835 | Train MAE: 0.0642 | Train R^2: 0.9430\n",
      "Validation Loss: 0.2025 | Validation RMSE: 0.4422 | Validation MAE: 0.2563 | Validation R^2: -0.9355\n",
      "\n",
      "Epoch 260/1000 | Train Loss: 0.0069 | Train RMSE: 0.0829 | Train MAE: 0.0643 | Train R^2: 0.9384\n",
      "Validation Loss: 0.2008 | Validation RMSE: 0.4414 | Validation MAE: 0.2714 | Validation R^2: -1.0947\n",
      "\n",
      "Epoch 280/1000 | Train Loss: 0.0064 | Train RMSE: 0.0802 | Train MAE: 0.0622 | Train R^2: 0.9442\n",
      "Validation Loss: 0.1978 | Validation RMSE: 0.4344 | Validation MAE: 0.2661 | Validation R^2: -0.8515\n",
      "\n",
      "Epoch 300/1000 | Train Loss: 0.0060 | Train RMSE: 0.0774 | Train MAE: 0.0600 | Train R^2: 0.9477\n",
      "Validation Loss: 0.2263 | Validation RMSE: 0.4689 | Validation MAE: 0.2724 | Validation R^2: -1.1316\n",
      "\n",
      "Epoch 320/1000 | Train Loss: 0.0057 | Train RMSE: 0.0757 | Train MAE: 0.0584 | Train R^2: 0.9490\n",
      "Validation Loss: 0.2341 | Validation RMSE: 0.4812 | Validation MAE: 0.2884 | Validation R^2: -1.6241\n",
      "\n",
      "Epoch 340/1000 | Train Loss: 0.0055 | Train RMSE: 0.0741 | Train MAE: 0.0575 | Train R^2: 0.9501\n",
      "Validation Loss: 0.2094 | Validation RMSE: 0.4496 | Validation MAE: 0.2642 | Validation R^2: -1.0660\n",
      "\n",
      "Epoch 360/1000 | Train Loss: 0.0051 | Train RMSE: 0.0715 | Train MAE: 0.0554 | Train R^2: 0.9565\n",
      "Validation Loss: 0.1712 | Validation RMSE: 0.4027 | Validation MAE: 0.2405 | Validation R^2: -0.5295\n",
      "\n",
      "Epoch 380/1000 | Train Loss: 0.0051 | Train RMSE: 0.0712 | Train MAE: 0.0550 | Train R^2: 0.9560\n",
      "Validation Loss: 0.1728 | Validation RMSE: 0.3820 | Validation MAE: 0.2395 | Validation R^2: -0.7253\n",
      "\n",
      "Epoch 400/1000 | Train Loss: 0.0049 | Train RMSE: 0.0701 | Train MAE: 0.0545 | Train R^2: 0.9565\n",
      "Validation Loss: 0.2137 | Validation RMSE: 0.4435 | Validation MAE: 0.2758 | Validation R^2: -1.0707\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mMSELoss()\n\u001b[1;32m      2\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mSGD(transformer\u001b[38;5;241m.\u001b[39mparameters(), lr \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.01\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtransformer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[10], line 28\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, epochs, optimizer, criterion, save_freq, print_results)\u001b[0m\n\u001b[1;32m     24\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     26\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m---> 28\u001b[0m rmse, mae, r_squared \u001b[38;5;241m=\u001b[39m \u001b[43mtesting_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m train_rmse \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m rmse\n\u001b[1;32m     30\u001b[0m train_mae \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m mae\n",
      "Cell \u001b[0;32mIn[5], line 12\u001b[0m, in \u001b[0;36mtesting_metrics\u001b[0;34m(logits_batch, target_batch)\u001b[0m\n\u001b[1;32m     10\u001b[0m     logits_t \u001b[38;5;241m=\u001b[39m logits_batch[:, t, :]\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     11\u001b[0m     target_t \u001b[38;5;241m=\u001b[39m target_batch[:, t, :]\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m---> 12\u001b[0m     r_squared_t \u001b[38;5;241m=\u001b[39m \u001b[43mr2_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogits_t\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     r_squared_values\u001b[38;5;241m.\u001b[39mappend(r_squared_t)\n\u001b[1;32m     15\u001b[0m r_squared_avg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(r_squared_values) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(r_squared_values)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.10/site-packages/sklearn/utils/_param_validation.py:214\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    210\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    211\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    212\u001b[0m         )\n\u001b[1;32m    213\u001b[0m     ):\n\u001b[0;32m--> 214\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    220\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    223\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    224\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.10/site-packages/sklearn/metrics/_regression.py:989\u001b[0m, in \u001b[0;36mr2_score\u001b[0;34m(y_true, y_pred, sample_weight, multioutput, force_finite)\u001b[0m\n\u001b[1;32m    848\u001b[0m \u001b[38;5;129m@validate_params\u001b[39m(\n\u001b[1;32m    849\u001b[0m     {\n\u001b[1;32m    850\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray-like\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    868\u001b[0m     force_finite\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    869\u001b[0m ):\n\u001b[1;32m    870\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\":math:`R^2` (coefficient of determination) regression score function.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m \n\u001b[1;32m    872\u001b[0m \u001b[38;5;124;03m    Best possible score is 1.0 and it can be negative (because the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    987\u001b[0m \u001b[38;5;124;03m    -inf\u001b[39;00m\n\u001b[1;32m    988\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 989\u001b[0m     y_type, y_true, y_pred, multioutput \u001b[38;5;241m=\u001b[39m \u001b[43m_check_reg_targets\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    990\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmultioutput\u001b[49m\n\u001b[1;32m    991\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    992\u001b[0m     check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[1;32m    994\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _num_samples(y_pred) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m2\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.10/site-packages/sklearn/metrics/_regression.py:100\u001b[0m, in \u001b[0;36m_check_reg_targets\u001b[0;34m(y_true, y_pred, multioutput, dtype)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Check that y_true and y_pred belong to the same regression task.\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \n\u001b[1;32m     68\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;124;03m    correct keyword.\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     99\u001b[0m check_consistent_length(y_true, y_pred)\n\u001b[0;32m--> 100\u001b[0m y_true \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    101\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m check_array(y_pred, ensure_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_true\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.10/site-packages/sklearn/utils/validation.py:957\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    951\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    952\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    953\u001b[0m             \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[1;32m    954\u001b[0m         )\n\u001b[1;32m    956\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[0;32m--> 957\u001b[0m         \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    958\u001b[0m \u001b[43m            \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    959\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    960\u001b[0m \u001b[43m            \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    961\u001b[0m \u001b[43m            \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    962\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    964\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_min_samples \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    965\u001b[0m     n_samples \u001b[38;5;241m=\u001b[39m _num_samples(array)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.10/site-packages/sklearn/utils/validation.py:110\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput contains NaN\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    109\u001b[0m \u001b[38;5;66;03m# We need only consider float arrays, hence can early return for all else.\u001b[39;00m\n\u001b[0;32m--> 110\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mxp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misdtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreal floating\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcomplex floating\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;66;03m# First try an O(n) time, O(1) space solution for the common case that\u001b[39;00m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;66;03m# everything is finite; fall back to O(n) space `np.isinf/isnan` or custom\u001b[39;00m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;66;03m# Cython implementation to prevent false positives and provide a detailed\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;66;03m# error message.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.10/site-packages/sklearn/utils/_array_api.py:283\u001b[0m, in \u001b[0;36m_NumPyAPIWrapper.isdtype\u001b[0;34m(self, dtype, kind)\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21misdtype\u001b[39m(\u001b[38;5;28mself\u001b[39m, dtype, kind):\n\u001b[0;32m--> 283\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43misdtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkind\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(transformer.parameters(), lr = 0.01)\n",
    "\n",
    "train_model(transformer, 1000, optimizer, criterion, save_freq=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
